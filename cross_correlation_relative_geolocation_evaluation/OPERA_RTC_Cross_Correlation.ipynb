{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8e4614-5c1f-4ff1-bfe7-42f8ebb1413b",
   "metadata": {},
   "source": [
    "# OPERA RTC Validation: Cross Correlation-based Relative Geolocation of a Stack\n",
    "\n",
    "**Alex Lewandowski, Eric Lundell, & Franz J Meyer; Alaska Satellite Facility, University of Alaska Fairbanks**\n",
    "\n",
    "This notebook analyzes the relative geolocation quality of OPERA RTC products using cross-correlation of images in a stack.\n",
    "\n",
    "Once Dask is initialized and tiffs are selected, the rest of the notebook can be run automatically. Once a section is run, subsequent sections can be rerun independently. This can reduce the non-linear aspects of notebooks and also allow for more playing with code.   \n",
    "\n",
    "The following procedures will be applied in this notebook:\n",
    "\n",
    "1. Select directory containing OPERA RTC mosaics prepared using the `OPERA_RTC_download_reproject_mosaic_sample_bursts.ipynb` notebook\n",
    "1. Tiffs in the selected polarization are copied to the same directory (`./vh/` or `./vv/`)\n",
    "1. Superset tiffs to a common AOI, in-place\n",
    "1. Due to spikes in data, flatten the tiffs by chopping off the top and bottom 1% of values. Save tiffs in `./{polarization}_flattened/`.\n",
    "1. Because of NaNs and other areas of no-data, evenly tile the tiffs (default to 8x8). To speed things up, we use a Dask LocalCluster to multiprocess. Tiles are saved in `./{polarization}_flattened_tiles/`.\n",
    "1. Apply the cross-correlation function to the individual tile nearest-chronological pairs. If more than 10% of a tile is NaNs, treat the whole tile as a NaN. Any remaining NaNs are converted to zero. Data is upscaled by a factor of ten. The cross correlation results include the shift in x and y and the RMSE. Results are converted from degrees to meters. The results are saved as json files for each tile pair in `./{polarization}_correlation/`.\n",
    "1. Perform analysis on json results. Results are read into a Pandas DataFrame. A statistical description and graph of the results are shown in two ways: all tiles in a scene are averaged and all tiles are averaged in time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01243e3-b9a4-459d-9806-c700aa0940a1",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfcdcfc-86ed-4827-bb02-6f2986e0afc1",
   "metadata": {},
   "source": [
    "# 0. OPERA RTC Relative Geolocation Requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502ddef-0967-40a6-a9dc-a5a733cc8bf9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<i>The Sentinel-1-based RTC product (RTC-S1) shall meet a relative geolocation accuracy better than or equal to 6 meters given the 30 meter RTC-S1 product resolution (i.e. 20% of the product resolution), excluding the effects of DEM errors, for at least 80% of all validation products considered.</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a28bb-57fc-4dca-804b-0b3c96ea7a6d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35af97-6c91-433e-aad9-dec1be04341a",
   "metadata": {},
   "source": [
    "# 1. Load Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88d746-0230-4f87-851f-b8b23094fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import re\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import dask.distributed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from scipy import stats\n",
    "from shapely import geometry\n",
    "from skimage.registration import phase_cross_correlation\n",
    "\n",
    "from ipyfilechooser import FileChooser\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import opensarlab_lib as asfn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88105df0-6ad4-4693-a33f-75c4dd11f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "METERS_PER_PIXEL = 30\n",
    "X_NUM = 8\n",
    "Y_NUM = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5010f-4f8d-4e20-9c40-98bc2754d3d2",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79aab9-be59-4995-95eb-d3d83329da9c",
   "metadata": {},
   "source": [
    "# 2. Setup Dask Methods\n",
    "\n",
    "Dask on a LocalCluster is used for multiprocessing to make some operations go faster. It is assumed that only one Dask client is used at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf0e0e-a000-4302-b3a6-4332b2178816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dask(ram_per_worker_gb:int=20, num_workers:int=20, num_threads_per_worker:int=1) -> dask.distributed.Client:\n",
    "    cluster = dask.distributed.LocalCluster(\n",
    "        threads_per_worker=num_threads_per_worker,\n",
    "        n_workers=num_workers,\n",
    "        memory_limit=f\"{ram_per_worker_gb}GB\",\n",
    "        processes=True\n",
    "    )\n",
    "\n",
    "    return dask.distributed.Client(cluster)\n",
    "\n",
    "def teardown_dask(client: dask.distributed.Client) -> None:\n",
    "    client.shutdown()\n",
    "\n",
    "def do_dask(client: dask.distributed.Client, callback, args: list):\n",
    "    try:\n",
    "        futures = client.map(callback, args)\n",
    "        dask.distributed.progress(futures)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in dask: {e}\")\n",
    "        teardown_dask(client)\n",
    "        return\n",
    "    \n",
    "    _  = client.gather(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cce5ebf-9025-4f4c-b3d5-b9224cba192f",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845e7b6-8bf0-46c7-821f-49c632e15bee",
   "metadata": {},
   "source": [
    "# 3. Select the directory holding your OPERA RTC sample data\n",
    "\n",
    "Choose the parent directory of all child directories that contain the desired stack of OPERA sample RTCs, which were downloaded and mosaiced with `OPERA_RTC_download_reproject_mosaic_sample_bursts.ipynb`\n",
    "\n",
    "```\n",
    "stack_directory ──\n",
    "                 │\n",
    "                 │─ OPERA_L2-RTC_*_30_v1.0 ──\n",
    "                 │                          │\n",
    "                 │                          │─  OPERA_L2_RTC-S1_VH*_30_v1.0_mosaic.tif\n",
    "                 │                          │─  OPERA_L2_RTC-S1_VV*_30_v1.0_mosaic.tif\n",
    "                 │    \n",
    "                 │─ OPERA_L2-RTC_*_30_v1.0 ──\n",
    "                 │                          │\n",
    "                 │                          │─  OPERA_L2_RTC-S1_VH*_30_v1.0_mosaic.tif\n",
    "                 │                          │─  OPERA_L2_RTC-S1_VV*_30_v1.0_mosaic.tif\n",
    "                 │          \n",
    "                 .\n",
    "                 .\n",
    "                 .\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2f5d1-55a0-4885-90dd-45ee98c385ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Select directory holding stack of mosaiced OPERA RTCs produced using OPERA_RTC_download_reproject_mosaic_sample_bursts.ipynb\")\n",
    "fc = FileChooser(Path.cwd())\n",
    "fc.show_only_dirs = True\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9371f-2b53-41d9-b655-5208510f69b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Select a polarization on which to run cross-correlation\") \n",
    "polar = asfn.select_parameter(['VH', 'VV'])\n",
    "display(polar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc2f80-8448-4fea-9bf0-d00be8b6a88b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# try/except for papermill\n",
    "try:\n",
    "    polarization = polar.value.lower()\n",
    "    stack_dir = Path(fc.selected)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e78466-5003-4328-8d76-f72445e984ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_dir = Path(stack_dir)\n",
    "\n",
    "output_dir = stack_dir.parent/f\"output_Coregistration\"\n",
    "\n",
    "polar_stack_dir = output_dir/f\"{polarization}\"\n",
    "\n",
    "polar_stack_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "tiff_og = list(stack_dir.glob(f\"*/OPERA_L2_RTC-S1_{polarization}*_30_v1.0_mosaic.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5082405-6f06-4ee6-adc9-3a6503bb6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tiff_og:\n",
    "    if not (polar_stack_dir/p.name).exists():\n",
    "        shutil.copy(p, polar_stack_dir/p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95172b4d-699a-4613-a616-40f8d5c502de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_pths = list(polar_stack_dir.glob(f\"OPERA_L2_RTC-S1_{polarization}*_30_v1.0_mosaic.tif\"))\n",
    "tiff_pths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974620b-64e6-4472-b197-97d91e8725dc",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ddfebc-8775-41aa-82f2-d08695914fcc",
   "metadata": {},
   "source": [
    "# 4. Superset OPERA RTC Images\n",
    "\n",
    "Scene frames have a tendency to move over time. This means that the extant coverage for the whole scene is always different per frame. For the cross-correlation to properly work and for more accurate comparison, all the scenes need to be \"normalized\" by increasing/decreasing the size of the square extant. \n",
    "\n",
    "From extant metadata, get the full superset coordinates for all stack scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c96865-9e4b-405e-9588-2c0beb586e6a",
   "metadata": {},
   "source": [
    "**Plot the first image in your stack to visualize some of the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c1ceb-cbbd-4cc1-96a2-91ad56ce3dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = rasterio.open(tiff_pths[0], mode='r')\n",
    "plt.imshow(src.read(1), cmap='pink', vmin=0.0, vmax=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31340f6-3946-4198-8e7e-c51f3cc7927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open all the tiffs and get overall coords.\n",
    "superset = {\n",
    "    'left': math.inf,\n",
    "    'bottom': math.inf,\n",
    "    'right': -math.inf,\n",
    "    'top': -math.inf\n",
    "}\n",
    "\n",
    "# The SRS is set to the first raster. It is assumed that the SRSs are the same for all.\n",
    "output_srs = None\n",
    "\n",
    "for i, original_path in enumerate(tiff_pths):\n",
    "\n",
    "    raster = rasterio.open(original_path)    \n",
    "    raster_bounds = raster.bounds\n",
    "    print(raster_bounds)\n",
    "    \n",
    "    if i == 0:\n",
    "        output_srs = raster.crs\n",
    "    \n",
    "    superset = {\n",
    "        'left': min(superset['left'], raster_bounds.left),\n",
    "        'bottom': min(superset['bottom'], raster_bounds.bottom), \n",
    "        'right': max(superset['right'], raster_bounds.right), \n",
    "        'top': max(superset['top'], raster_bounds.top)\n",
    "    }\n",
    "\n",
    "print(f\"Superset box coords: {superset}\")\n",
    "print(f\"Output SRS: {output_srs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6229d-3de8-4d11-ab62-897b2a64c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bounds = (\n",
    "            superset['left'], \n",
    "            superset['bottom'],\n",
    "            superset['right'],\n",
    "            superset['top'],\n",
    "        )\n",
    "\n",
    "print(f\"Output bounds (superset) set to '{output_bounds}'\")\n",
    "print(f\"Output SRS set to '{output_srs}'\")\n",
    "\n",
    "# Superset and save tiffs\n",
    "for original_path in tqdm(tiff_pths):\n",
    "   \n",
    "    gdal.Warp(\n",
    "        str(original_path),\n",
    "        str(original_path), \n",
    "        outputBounds=output_bounds,\n",
    "        outputBoundsSRS=output_srs,\n",
    "        xRes=30.0, \n",
    "        yRes=30.0, \n",
    "        targetAlignedPixels=True,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0a459-a554-4b95-93b8-ec1055b4ea69",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064dd33-addc-4526-abd8-2f20b43a5882",
   "metadata": {},
   "source": [
    "# 5. Flatten and Save RTCs\n",
    "\n",
    "Often, the RTCs have extraneous high and low values that make matching difficult. So we need to get rid of these and save the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac638fa-8e96-435d-85a0-51a0797c16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_dir = output_dir/f\"{polarization}_flattened\"\n",
    "flat_choice = None\n",
    "if len(list(flatten_dir.glob(\"*.tif*\"))) > 0:\n",
    "    print(\"Do you wish to skip flattening, add flattened tiffs to the directory, or delete and replace the contents of the directory?\")\n",
    "    flat_choice = asfn.select_parameter([\"skip flattening\", \"add flattened layers\", \"delete and replace flattened layers\"])\n",
    "    display(flat_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a98582-7ef5-4480-a41b-fa97415abfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {flatten_dir}\n",
    "\n",
    "if flat_choice and 'delete' in flat_choice.value:\n",
    "    # Remove any staged intermediate files to work in a clean area\n",
    "    for filepath in flatten_dir.glob(\"*.tif*\"):\n",
    "        filepath.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc445a32-52ff-4603-91fc-bf07383ed2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Truncated values become NaNs\n",
    "    \"\"\"\n",
    "    df[df < np.nanpercentile(df, 1)] = np.nan\n",
    "    df[df > np.nanpercentile(df, 99)] = np.nan\n",
    "    return df\n",
    "\n",
    "if not flat_choice or \"add\" in flat_choice.value or \"delete\" in flat_choice.value:\n",
    "\n",
    "    for p in tqdm(tiff_pths):\n",
    "        print(f\"Flattening {p}\")\n",
    "\n",
    "        # Convert raster to dataframe\n",
    "        raster = rasterio.open(p)\n",
    "        raster_metadata = raster.meta\n",
    "\n",
    "        raster0 = raster.read(1)\n",
    "        df = pd.DataFrame(raster0)\n",
    "\n",
    "        # Flatten raster data\n",
    "        df_flatten = flatten(df)\n",
    "\n",
    "        flatten_path = flatten_dir/f\"{p.stem}_flat.tif\"\n",
    "\n",
    "        with rasterio.open(flatten_path, 'w', **raster_metadata) as out:\n",
    "            out.write(df_flatten, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ebef9-3abf-44d9-bad1-9c28fc55518d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf64fa-e58e-4b07-bdfb-fcca460567ee",
   "metadata": {},
   "source": [
    "# 6. Tile and Save GeoTiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394ba90-1ffc-47d2-94cd-62db426fb6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from rasterio.windows import Window\n",
    "\n",
    "def split_into_cells_args(x_num: int, y_num: int, tiff_pths: List[Path], output_dir: Path) -> List:\n",
    "    \"\"\"\n",
    "    return list of dict of args for `split_into_cells` dask function callback.\n",
    "    \"\"\"\n",
    "    \n",
    "    args = []\n",
    "    for i, flatten_path in enumerate(tiff_pths):\n",
    "        args.append({\n",
    "            'input_number': i, \n",
    "            'input_file': flatten_path, \n",
    "            'output_dir': output_dir, \n",
    "            'x_num': x_num, \n",
    "            'y_num': y_num\n",
    "        })\n",
    "    \n",
    "    return args \n",
    "\n",
    "# https://gis.stackexchange.com/a/306862\n",
    "# Takes a Rasterio dataset and splits it into squares of dimensions squareDim * squareDim\n",
    "def split_into_cells(args):\n",
    "    \"\"\"\n",
    "    input_number: A sequential number representing the ordering of the scenes. This is to make later scene pairing easier.\n",
    "    input_file: Full file path of scene to be tiled.\n",
    "    output_dir: Full path of directory to place tiles.\n",
    "    x_num: Number of tiles formed in the x direction per scene.\n",
    "    y_num: Number of tiles formed in the y direction per scene.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_number: int = args['input_number']\n",
    "    input_file: str = args['input_file']\n",
    "    output_dir: str = args['output_dir']\n",
    "    x_num: int = args.get('x_num', 1)\n",
    "    y_num: int = args.get('y_num', 1)\n",
    "    \n",
    "    print(f\"Tileing {input_file}\")\n",
    "\n",
    "    \n",
    "    raster = rasterio.open(input_file)\n",
    "    \n",
    "    x_dim = raster.shape[1] // x_num\n",
    "    y_dim = raster.shape[0] // y_num\n",
    "\n",
    "    x, y = 0, 0\n",
    "    for i, y_iter in enumerate(range(y_num)):\n",
    "        y = y_iter * y_dim\n",
    "        for x_iter in range(x_num):\n",
    "            x = x_iter * x_dim\n",
    "            \n",
    "            input_filestem = Path(input_file).stem\n",
    "            \n",
    "            output_file = f'{input_filestem}_{input_number}_{y_iter}_{x_iter}.tif'\n",
    "            print(f\"Creating tile {output_file}...\")\n",
    "            \n",
    "            # Get tile geometry\n",
    "            corner1 = raster.transform * (x, y)\n",
    "            corner2 = raster.transform * (x + x_dim, y + y_dim)\n",
    "            geom = geometry.box(corner1[0], corner1[1], corner2[0], corner2[1])\n",
    "            \n",
    "            # Get cell \n",
    "            crop, cropTransform = mask(raster, [geom], crop=True)\n",
    "            \n",
    "            meta_args = {\n",
    "                'dtype': 'float32',\n",
    "                'nodata': np.nan,\n",
    "                'count': 1,\n",
    "                \"driver\": \"GTiff\",\n",
    "                \"height\": crop.shape[1],\n",
    "                \"width\": crop.shape[2],\n",
    "                \"transform\": cropTransform,\n",
    "                \"crs\": raster.crs\n",
    "            }\n",
    "                        \n",
    "            output_filepath = f\"{output_dir}/{output_file}\"\n",
    "            with rasterio.open(output_filepath, \"w\", **meta_args) as out:\n",
    "                out.write(crop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1050bf7d-4426-4a1d-bcec-c12e68a6e321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tile_dir = output_dir/f\"{polarization}_flattened_tiles\"\n",
    "tile_choice = None\n",
    "if len(list(tile_dir.glob(\"*.tif*\"))) > 0:\n",
    "    print(\"Do you wish to skip tiling, add tiles, or delete and replace the contents of the tile directory?\")\n",
    "    tile_choice = asfn.select_parameter([\"skip tiling\", \"add tiles\", \"delete and replace tiles\"])\n",
    "    display(tile_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71988e1d-162c-40e3-8d48-1f86748fac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tile_dir.mkdir()\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "if tile_choice and 'delete' in tile_choice.value:\n",
    "    # Remove any staged intermediate files to work in a clean area\n",
    "    for filepath in tile_dir.glob(\"*.tif*\"):\n",
    "        filepath.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c818293-dfcd-4243-b3db-a9610319ef45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not tile_choice or \"add\" in tile_choice.value or \"delete\" in tile_choice.value:\n",
    "    \n",
    "\n",
    "    tiff_pths = list(flatten_dir.glob(f\"*{polarization}*.tif\"))\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(f\"\\nStart time is {start_time}\")\n",
    "\n",
    "    client = setup_dask(ram_per_worker_gb=20, num_workers=100, num_threads_per_worker=1)\n",
    "    do_dask(client, split_into_cells, split_into_cells_args(x_num=X_NUM, y_num=Y_NUM, tiff_pths=tiff_pths, output_dir=tile_dir))\n",
    "\n",
    "    teardown_dask(client)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(f\"\\nEnd time is {end_time}\")\n",
    "    print(f\"Time elapsed is {end_time - start_time}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c02e7-643f-4e73-b142-f3cfa0a5809a",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c8f60-94de-462d-862c-7995dc61ee46",
   "metadata": {},
   "source": [
    "# 7. Correlate Tiles and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31e35a-3b89-4f21-bc5a-db3708234901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_correlation_args(corr_arg_list, ref_row, sec_row):\n",
    "    for j in range(X_NUM*Y_NUM-1):\n",
    "        ref = ref_row.iloc[j]\n",
    "        sec = sec_row.iloc[j]   \n",
    "\n",
    "        corr_arg_list.append({\n",
    "            'reference_index': ref['tile_index'],\n",
    "            'secondary_index': sec['tile_index'],\n",
    "            'tile_number_x': ref['tile_number_x'],\n",
    "            'tile_number_y': ref['tile_number_y'],\n",
    "            'ref_file_path': ref['file_path'],\n",
    "            'sec_file_path': sec['file_path']\n",
    "        })\n",
    "\n",
    "def get_correlation_args(tiles_paths, first_last=False, additional_step=1) -> list:\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'reference_index': '',\n",
    "            'secondary_index': '',\n",
    "            'tile_number_x': '',\n",
    "            'tile_number_y': '',\n",
    "            'ref_file_path': '',\n",
    "            'sec_file_path': ''\n",
    "        },\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    tiles = []\n",
    "    \n",
    "    # Get index and tile numbers from path\n",
    "    for tiles_path in tiles_paths:\n",
    "\n",
    "        m = re.match(r\".*_([0-9]+)_([0-9]+)_([0-9]+).tif*\", tiles_path.name)\n",
    "\n",
    "        tiles.append({\n",
    "            'tile_index': m.group(1),\n",
    "            'tile_number_x': m.group(2),\n",
    "            'tile_number_y': m.group(3),\n",
    "            'file_path': tiles_path\n",
    "        })\n",
    "\n",
    "    tiles_df = pd.DataFrame(tiles).sort_values(by=['tile_index', 'tile_number_x', 'tile_number_y'])\n",
    "\n",
    "    corr_arg_list = []\n",
    "    scene_count = len(set(tiles_df.tile_index))\n",
    "    \n",
    "    for i in range(scene_count-1):\n",
    "        ref_row = tiles_df.loc[tiles_df['tile_index'] == str(i)]\n",
    "        sec_row = tiles_df.loc[tiles_df['tile_index'] == str(i+1)]\n",
    "        append_correlation_args(corr_arg_list, ref_row, sec_row)\n",
    "          \n",
    "    if first_last:\n",
    "        ref_row = tiles_df.loc[tiles_df['tile_index'] == str(0)]\n",
    "        sec_row = tiles_df.loc[tiles_df['tile_index'] == str(scene_count-1)]\n",
    "        append_correlation_args(corr_arg_list, ref_row, sec_row)\n",
    "        \n",
    "    if additional_step > 1:\n",
    "        for i in range(0, scene_count-additional_step, additional_step):\n",
    "            ref_row = tiles_df.loc[tiles_df['tile_index'] == str(i)]\n",
    "            sec_row = tiles_df.loc[tiles_df['tile_index'] == str(i+additional_step)]    \n",
    "            append_correlation_args(corr_arg_list, ref_row, sec_row)\n",
    "\n",
    "    return corr_arg_list\n",
    "\n",
    "def correlation_callback(args: dict) -> dict:\n",
    "    \"\"\"\n",
    "    args = {\n",
    "        'reference_index': '',\n",
    "        'secondary_index': '',\n",
    "        'tile_number_x': '',\n",
    "        'tile_number_y': '',\n",
    "        'ref_file_path': '',\n",
    "        'sec_file_path': ''\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        reference_index = args['reference_index']\n",
    "        secondary_index = args['secondary_index']\n",
    "        tile_number_x = args['tile_number_x']\n",
    "        tile_number_y = args['tile_number_y']\n",
    "        ref_file_path = args['ref_file_path']\n",
    "        sec_file_path = args['sec_file_path']\n",
    "        \n",
    "        ###### Reference \n",
    "        stime = datetime.now()\n",
    "        print(f\"\\nIndex {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Rendering {ref_file_path}...\")\n",
    "        rast = rasterio.open(ref_file_path)\n",
    "        raster0 = rast.read(1)\n",
    "        df_ref = pd.DataFrame(raster0)\n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Time to complete ref: {datetime.now() - stime}\")\n",
    "\n",
    "\n",
    "        ###### Secondary\n",
    "        stime = datetime.now()\n",
    "        print(f\"\\nIndex {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Rendering {sec_file_path}...\")\n",
    "        rast = rasterio.open(sec_file_path)\n",
    "        raster0 = rast.read(1)\n",
    "        df_sec = pd.DataFrame(raster0)\n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Time to complete sec: {datetime.now() - stime}\")\n",
    "\n",
    "\n",
    "        ###### If crop tile is more than 10% NANs, skip correlation and set return values to NaN \n",
    "        def get_percent_nans(df):\n",
    "            number_of_elements = df.size\n",
    "            number_of_nans = df.isnull().sum().sum()\n",
    "\n",
    "            return number_of_nans / number_of_elements\n",
    "\n",
    "        percent_nans_ref = get_percent_nans(df_ref)\n",
    "        percent_nans_sec = get_percent_nans(df_sec)\n",
    "\n",
    "        if percent_nans_ref > 0.10 or percent_nans_sec > 0.10:\n",
    "            print(f\"\\nIndex {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Too many NaNs. Skipping correlation....\")\n",
    "\n",
    "            result = {\n",
    "                \"reference_index\": int(reference_index),\n",
    "                \"secondary_index\": int(secondary_index),\n",
    "                \"tile_number_x\": int(tile_number_x),\n",
    "                \"tile_number_y\": int(tile_number_y),\n",
    "                \"ref_file\": str(ref_file_path),\n",
    "                \"sec_file\": str(sec_file_path),\n",
    "                \"shift_x\": np.nan,\n",
    "                \"shift_y\": np.nan, \n",
    "                \"error\": np.nan, \n",
    "                \"phase\": np.nan,\n",
    "                \"message\": \"Too many NaNs\"\n",
    "            }\n",
    "\n",
    "        ####### Cross corr without masking\n",
    "        stime = datetime.now()\n",
    "        print(f\"\\nIndex {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Finding phase correlation with nans set to zero....\")\n",
    "        shift, error, phase = phase_cross_correlation(\n",
    "            df_ref.replace(np.nan, 0), \n",
    "            df_sec.replace(np.nan, 0),\n",
    "            normalization=None,\n",
    "            upsample_factor=10\n",
    "        )\n",
    "        \n",
    "        shift = shift * METERS_PER_PIXEL\n",
    "        error = error * METERS_PER_PIXEL\n",
    "        \n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Shift vector (in meters) required to register moving_image with reference_image: {shift}\")\n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Translation invariant normalized RMS error between reference_image and moving_image: {error}\")\n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Global phase difference between the two images (should be zero if images are non-negative).: {phase}\\n\")\n",
    "\n",
    "        if len(list(shift)) != 2:\n",
    "            result = {\n",
    "                \"reference_index\": int(reference_index),\n",
    "                \"secondary_index\": int(secondary_index),\n",
    "                \"tile_number_x\": int(tile_number_x),\n",
    "                \"tile_number_y\": int(tile_number_y),\n",
    "                \"ref_file\": str(ref_file_path),\n",
    "                \"sec_file\": str(sec_file_path),\n",
    "                \"shift_x\": np.float64(shift[0]),\n",
    "                \"shift_y\": np.float64(shift[1]),\n",
    "                \"error\": np.nan, \n",
    "                \"phase\": np.nan,\n",
    "                \"message\": \"Shift is not a two element array\"\n",
    "            }\n",
    "        \n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}:  Time to complete correlation: {datetime.now() - stime}\")\n",
    "\n",
    "\n",
    "        ####### Write metadata to correlation result files\n",
    "\n",
    "        result = {\n",
    "            \"reference_index\": int(reference_index),\n",
    "            \"secondary_index\": int(secondary_index),\n",
    "            \"tile_number_x\": int(tile_number_x),\n",
    "            \"tile_number_y\": int(tile_number_y),\n",
    "            \"ref_file\": str(ref_file_path),\n",
    "            \"sec_file\": str(sec_file_path),\n",
    "            \"shift_x\": np.float64(shift[0]),\n",
    "            \"shift_y\": np.float64(shift[1]),\n",
    "            \"error\": np.float64(error), \n",
    "            \"phase\": np.float64(phase),\n",
    "            \"message\": \"Correlation successful\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        result = {\n",
    "            \"reference_index\": int(reference_index),\n",
    "            \"secondary_index\": int(secondary_index),\n",
    "            \"tile_number_x\": int(tile_number_x),\n",
    "            \"tile_number_y\": int(tile_number_y),\n",
    "            \"ref_file\": str(ref_file_path),\n",
    "            \"sec_file\": str(sec_file_path),\n",
    "            \"shift_x\": np.nan, \n",
    "            \"shift_y\": np.nan,\n",
    "            \"error\": np.nan, \n",
    "            \"phase\": np.nan,\n",
    "            \"message\": f\"Error: {e}\"\n",
    "        }\n",
    "        \n",
    "    try:\n",
    "        result_file = correlation_dir/f\"index_{reference_index}_{secondary_index}-tile_{tile_number_x}_{tile_number_y}.json\"\n",
    "        with open(result_file, 'w') as f:\n",
    "            json.dump(result, f)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27a61f2-dabf-4b38-ba0e-52c8b2cd049e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlation_dir = output_dir/f\"{polarization}_correlation\"\n",
    "correlation_choice = None\n",
    "if len(list(correlation_dir.glob(\"*.tif*\"))) > 0:\n",
    "    print(\"Do you wish to skip correlation, add correlation results, or delete and replace the correlation results?\")\n",
    "    correlation_choice = asfn.select_parameter([\"skip correlation\", \"add correlation results\", \"delete and replace correlation results\"])\n",
    "    display(correlation_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173d3d0-320e-4100-8509-5929110e6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {correlation_dir}\n",
    "\n",
    "if correlation_choice and 'delete' in correlation_choice.value:\n",
    "    # Remove any staged intermediate files to work in a clean area\n",
    "    for filepath in correlation_dir.glob(\"*.tif*\"):\n",
    "        filepath.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed10930-54c8-40b0-ac0a-270077af4360",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not correlation_choice or \"add\" in correlation_choice.value or \"delete\" in correlation_choice.value:\n",
    "    flat_tif_pth = list(tile_dir.glob(\"*tif*\"))\n",
    "    flat_tif_pth\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(f\"\\nStart time is {start_time}\")\n",
    "\n",
    "    # ram_per_worker_gb:int=20, num_workers:int=20, num_threads_per_worker:int=1\n",
    "    client = setup_dask(ram_per_worker_gb=11, num_workers=10)\n",
    "    do_dask(client, correlation_callback, get_correlation_args(flat_tif_pth, first_last=True, additional_step=4))\n",
    "\n",
    "    teardown_dask(client)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(f\"\\nEnd time is {end_time}\")\n",
    "    print(f\"Time elapsed is {end_time - start_time}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18626600-6aec-4785-b891-0941f6044b95",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c76bba-9d9b-4a18-9feb-19edd74ede06",
   "metadata": {},
   "source": [
    "# 8. Estimate Average Offsets Per Tile and For the Full Scene\n",
    "\n",
    "Read the correlation result files from the previous section into a Pandas DataFrame.\n",
    "\n",
    "Then perform various statistical analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f86ad8-b5d5-4ad5-b740-c069e14dfdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put offset results into 3D Pandas dataset\n",
    "correlation_paths = correlation_dir.glob(f\"*.json\")\n",
    "\n",
    "offset_result_list = []\n",
    "\n",
    "for corr_path in correlation_paths:\n",
    "    with open(corr_path, 'r') as f:\n",
    "        offset_result_list.append(json.load(f))\n",
    "\n",
    "offset_result_df = pd.DataFrame(offset_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621e8b4-43af-4165-955b-68a09540639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(offset_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066c160-3a6d-45ae-9afe-914458c93898",
   "metadata": {},
   "source": [
    "Display the stats for the shift, error, and phase of the cross-correlation between two scenes in the stack.\n",
    "\n",
    "The `reference_index` is the order number of the reference scene within the stack. The stack scenes are ordered from newest to oldest.\n",
    "The `secondary_index` is the order number of the secondary scene within the stack.\n",
    "\n",
    "The `mean` value is a simple mean of all tile values. Similarity for `std`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae015a0-6ead-4ce6-b732-3deb103475e0",
   "metadata": {},
   "source": [
    "## Combine all tiles per scene pair correlation results\n",
    "\n",
    "The cross-correlation results of all tiles in Scene 1 and Scene 2 are re-assembled together into one result and statistically analyzed. Repeat for all pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824822fe-015b-4b70-8f82-393baf397afb",
   "metadata": {},
   "source": [
    "**First, remove invalid matches:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472e1e9-ef4a-41ec-ba26-89be63b64de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_result_df['error'] = offset_result_df[offset_result_df['error'] < 25.0]['error']\n",
    "offset_result_df['shift_x'] = offset_result_df[np.abs(offset_result_df['shift_x']) < 50.0]['shift_x']\n",
    "offset_result_df['shift_y'] = offset_result_df[np.abs(offset_result_df['shift_y']) < 50.0]['shift_y']\n",
    "no_nans = offset_result_df[~offset_result_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a393fd-b403-4a7e-a8fd-31c82864fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6030154-9956-4dab-945a-f265568c873c",
   "metadata": {},
   "source": [
    "**Calculate average scene-wide offsets along with their errors:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b4e4f-7e51-4062-8db3-a0e613e84f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_offset_df = no_nans.groupby(by=['reference_index', 'secondary_index'])\n",
    "\n",
    "## Uncomment to display statistical descriptions of DataFrames\n",
    "# display(pair_offset_df['shift_x'].describe())\n",
    "# display(pair_offset_df['shift_y'].describe())\n",
    "# display(pair_offset_df['error'].describe())\n",
    "\n",
    "display(pair_offset_df)\n",
    "\n",
    "# Take the standard error of mean of the individual tile RMSE to get the overall RMSE.\n",
    "def sem(series):\n",
    "    if np.isnan(series).all():\n",
    "        return np.nan\n",
    "    return stats.sem(series, nan_policy='omit', axis=None)\n",
    "\n",
    "aggregated_results_df = pd.DataFrame()\n",
    "aggregated_results_df['tile_mean_x'] = pair_offset_df['shift_x'].agg(['mean'])\n",
    "aggregated_results_df['tile_mean_y'] = pair_offset_df['shift_y'].agg(['mean'])\n",
    "aggregated_results_df['error'] = pair_offset_df['error'].agg(sem)\n",
    "display(aggregated_results_df)\n",
    "\n",
    "super_mean_per_pair_tile_mean_x = np.nanmean(aggregated_results_df['tile_mean_x'])\n",
    "super_mean_per_pair_tile_mean_y = np.nanmean(aggregated_results_df['tile_mean_y'])\n",
    "std_per_pair_tile_mean_x = np.nanstd(aggregated_results_df['tile_mean_x'])\n",
    "std_per_pair_tile_mean_y = np.nanstd(aggregated_results_df['tile_mean_y'])\n",
    "\n",
    "print(f\"Mean of tile_mean_x across all scenes: {super_mean_per_pair_tile_mean_x}\")\n",
    "print(f\"Mean of tile_mean_y across all scenes: {super_mean_per_pair_tile_mean_y}\")\n",
    "\n",
    "print(f\"STD of tile_mean_x across all scenes: {std_per_pair_tile_mean_x}\")\n",
    "print(f\"STD of tile_mean_y across all scenes: {std_per_pair_tile_mean_y}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "requirement = plt.Rectangle((-3.0,-3.0), 6.0, 6.0, fill=False, edgecolor='grey', label='Requirement')\n",
    "ax.add_patch(requirement)\n",
    "plt.grid(color='grey', alpha=0.4)\n",
    "\n",
    "plt.errorbar(aggregated_results_df['tile_mean_x'], aggregated_results_df['tile_mean_y'], \n",
    "             yerr=aggregated_results_df['error'], xerr=aggregated_results_df['error'], \n",
    "             ecolor='green', alpha=0.3, ls='none')\n",
    "\n",
    "plt.scatter(aggregated_results_df['tile_mean_x'], aggregated_results_df['tile_mean_y'],\n",
    "            color='green', label='Offset Estimates')\n",
    "\n",
    "plt.errorbar([super_mean_per_pair_tile_mean_x], [super_mean_per_pair_tile_mean_y], \n",
    "             yerr=[std_per_pair_tile_mean_y], xerr=[std_per_pair_tile_mean_x], \n",
    "             ecolor='red', alpha=0.3, ls='none')\n",
    "\n",
    "plt.scatter([super_mean_per_pair_tile_mean_x], [super_mean_per_pair_tile_mean_y], \n",
    "            color='red', label='Mean of Offset Estimates')\n",
    "\n",
    "ax.legend()\n",
    "plt.xlabel(\"Easting Offset (Meters)\")\n",
    "plt.ylabel(\"Northing Offset (Meters)\")\n",
    "plt.title(\"Cross-correlation Offset Per Stack Pair w/ RMSE\")\n",
    "\n",
    "plt.savefig(correlation_dir/f'CrossCorrelationOffsets-Per_Pair_{polarization}_requirements.png', dpi=300, transparent='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04687d30-c707-4880-9440-51093d99684f",
   "metadata": {},
   "source": [
    "# 9. Write results to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e2fd5-7535-4f2c-ac41-c32040b0902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_pair_csv = output_dir/f\"{stack_dir.name}_per_pair_tile_offset_means.csv\"\n",
    "\n",
    "fields = [\"stack\", \"polarization\", \"per pair mean tile mean x\", \"per pair mean tile mean y\", \"per pair STD tile mean x\", \"per pair STD tile mean y\"]\n",
    "stack = ' '.join([p.name for p in tiff_pths])\n",
    "\n",
    "row = [stack, polarization, \n",
    "       super_mean_per_pair_tile_mean_x, super_mean_per_pair_tile_mean_y,\n",
    "       std_per_pair_tile_mean_x, std_per_pair_tile_mean_y]\n",
    "\n",
    "if not per_pair_csv.exists():\n",
    "    with open(per_pair_csv, 'w') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(fields)\n",
    "        csvwriter.writerow(row)\n",
    "else:\n",
    "    with open(per_pair_csv, 'r') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        stacks = [c[0] for c in list(csvreader)]\n",
    "    if stack not in stacks:\n",
    "        with open(per_pair_csv, 'a') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5e205-9e12-4a46-888b-fc16a186c977",
   "metadata": {},
   "source": [
    "# 10. Clean up input data and intermediate products (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d099b-9063-4199-9b53-3f7f5d755a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try/except for Papermill\n",
    "try:\n",
    "    if type(delete_mosaics) == bool:\n",
    "        pass\n",
    "except NameError:\n",
    "    print(\"Do you wish to save or delete the mosaicked RTCs and static files?\")\n",
    "    mosaic_cleanup = widgets.RadioButtons(\n",
    "            options=['save mosaics', 'delete mosaics'],\n",
    "            disabled=False,\n",
    "        )\n",
    "    display(mosaic_cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7d795-6a5a-497d-bef9-9c222fa1c6fd",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# try/except for Papermill\n",
    "try:\n",
    "    delete_mosaics = \"delete\" in mosaic_cleanup.value\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8287b77-ee40-4123-b910-02f1bf8f46cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_paths(path_list):\n",
    "    for p in path_list:\n",
    "        p.unlink()\n",
    "\n",
    "if delete_mosaics:\n",
    "    inc_angle = list(stack_dir.glob(f\"*/OPERA_RTC_v0.4_inc_angle_*_mosaic.tif\"))\n",
    "    local_inc_angle = list(stack_dir.glob(f\"*/OPERA_RTC_v0.4_local_inc_angle_*_mosaic.tif\"))\n",
    "    ls_mask = list(stack_dir.glob(f\"*/OPERA_RTC_v0.4_ls_mask_*_mosaic.tif\"))\n",
    "    for f in [tiff_og, inc_angle, local_inc_angle, ls_mask]:\n",
    "        delete_paths(f)\n",
    "        \n",
    "    # delete dirs only if empty\n",
    "    mosaic_dir_list = list(stack_dir.glob('*'))\n",
    "    mosaic_dir_list.append(stack_dir)\n",
    "    for d in mosaic_dir_list:\n",
    "        try:\n",
    "            d.rmdir()\n",
    "        except OSError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3f48f-b4fc-499f-a098-75144c193b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type_dict = {\n",
    "    f\"{polarization} amplitude data\": polar_stack_dir,\n",
    "    f\"flattened {polarization} amplitude data\": flatten_dir,\n",
    "    f\"flattened and tiled {polarization} amplitude data\": tile_dir,\n",
    "    f\"{polarization} tile correlation results\": correlation_dir\n",
    "}\n",
    "\n",
    "print(\"Select file types to delete from output directory\")\n",
    "      \n",
    "output_cleanup = widgets.SelectMultiple(\n",
    "    options=file_type_dict,\n",
    "    disabled=False,\n",
    ")\n",
    "display(output_cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff6b4f-b15e-4a3c-bdd0-e0a260cad7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # raw string passed from Papermill\n",
    "    cleanup_list = [l for l in cleanup_list.split(', ') if l != '']\n",
    "except NameError:\n",
    "    cleanup_list = list(output_cleanup.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125ce8b-49cd-4a9e-8bdf-5478d6e59d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle list as a raw string passed from Papermill\n",
    "print(cleanup_list)\n",
    "if type(cleanup_list) == str and len(cleanup_list) > 0:\n",
    "    cleanup_list = [l for l in cleanup_list.split(', ') if l != '']\n",
    "    print(cleanup_list)\n",
    "\n",
    "    for k in cleanup_list:\n",
    "        shutil.rmtree(file_type_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182680c8-a221-4c2c-b713-1fce6da968ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opera_calval_rtc [conda env:.local-opera_calval_rtc]",
   "language": "python",
   "name": "conda-env-.local-opera_calval_rtc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
