{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8e4614-5c1f-4ff1-bfe7-42f8ebb1413b",
   "metadata": {},
   "source": [
    "# Check Cross Correlation of a Stack\n",
    "\n",
    "** This notebook is best ran on a machine that can handle 120 GB RAM and 32 CPUs **\n",
    "\n",
    "The following notebook assumes that a stack of RTC scenes have been created and put into the `./work/original/` directory (possibly through the accompanying `1b_get_prepared_data_from_s3` notebook).\n",
    "\n",
    "Once imports are imported, Dask is initialized (section 0), and VVs are selected (section 1), the rest of the notebook can be run automatically. Once a section is run, a sequential section can be reran independently. This can reduce the non-linear aspects of notebooks and also allow for more playing with code.   \n",
    "\n",
    "The following procedures will be applied in this notebook:\n",
    "\n",
    "1. Select VV tiffs from prepared stack. Any `*_vv.tif` within the selected directory or child directories will be copied. Move tiffs to `./.work/original/`.\n",
    "1. Superset tiffs to a common AOI and save re-formatted tiffs in `./work/superset/`.\n",
    "1. Due to spikes in data, flatten the tiffs by chopping off the bottom 1% and top 1%. Save tiffs in `./work/flatten/`.\n",
    "1. Because of NaNs and other areas of no-data, evenly tile the tiffs (default to 8x8). To speed things up, we use a Dask LocalCluster to multiprocess. Tiles are saved in `./work/tiles/`.\n",
    "1. Apply the cross-correlation function to the individual tile nearest-chronological pairs. If more than 10% of a tile is NaNs, treat the whole tile as a NaN. Any remaining NaNs are converted to zero. Data is upscaled by a factor of ten. The cross correlation results include the shift in x and y and the RMSE. Results are converted from degrees to meters. The results are saved as json files for each tile pair in `./work/correlation/`.\n",
    "1. Perform analysis on json results. Results are read into a Pandas DataFrame. A statistical description and graph of the results are shown in two ways: all tiles in a scene are averaged and all tiles are averaged in time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35af97-6c91-433e-aad9-dec1be04341a",
   "metadata": {},
   "source": [
    "### Some Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88d746-0230-4f87-851f-b8b23094fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import math\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "\n",
    "from ipyfilechooser import FileChooser\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from shapely import geometry\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.distributed\n",
    "from skimage.registration import phase_cross_correlation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import opensarlab_lib as asfn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Get working directory of notebook\n",
    "CWD = pathlib.Path().cwd()\n",
    "CWD\n",
    "\n",
    "METERS_PER_PIXEL = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79aab9-be59-4995-95eb-d3d83329da9c",
   "metadata": {},
   "source": [
    "### 0. Setup Dask Methods\n",
    "\n",
    "Dask on a LocalCluster is used for multiprocessing to make some operations go faster. It is assumed that only one Dask client is used at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf0e0e-a000-4302-b3a6-4332b2178816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dask(ram_per_worker_gb:int=20, num_workers:int=20, num_threads_per_worker:int=1) -> dask.distributed.Client:\n",
    "    cluster = dask.distributed.LocalCluster(\n",
    "        threads_per_worker=num_threads_per_worker,\n",
    "        n_workers=num_workers,\n",
    "        memory_limit=f\"{ram_per_worker_gb}GB\",\n",
    "        processes=True\n",
    "    )\n",
    "\n",
    "    return dask.distributed.Client(cluster)\n",
    "\n",
    "def teardown_dask(client: dask.distributed.Client) -> None:\n",
    "    client.shutdown()\n",
    "\n",
    "def do_dask(client: dask.distributed.Client, callback, args: list):\n",
    "    try:\n",
    "        futures = client.map(callback, args)\n",
    "        dask.distributed.progress(futures)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in dask: {e}\")\n",
    "        teardown_dask(client)\n",
    "        return\n",
    "    \n",
    "    _  = client.gather(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845e7b6-8bf0-46c7-821f-49c632e15bee",
   "metadata": {},
   "source": [
    "### 1. Select VVs\n",
    "\n",
    "Choose the parent directory of all child directories that contain the desired stack of VVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2f5d1-55a0-4885-90dd-45ee98c385ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser(f'{CWD}/data/')\n",
    "fc.show_only_dirs = True\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8433d-98dd-4cd3-8444-bf56399fa810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any staged intermediate files to work in a clean area\n",
    "!mkdir -p \"{CWD}/work/original\"\n",
    "for filepath in pathlib.Path(f\"{CWD}/work/original\").glob(\"*.tif\"):\n",
    "    filepath.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67db51-5826-4434-b8e7-646c9dabfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vv_paths = pathlib.Path(fc.selected_path).glob(\"**/*_VV.tif\")\n",
    "\n",
    "# Move desired products to work directory.\n",
    "for source_path in all_vv_paths:\n",
    "    print(f\"Copying {source_path} to {CWD}/work/original/{source_path.name}\")\n",
    "    !cp \"{source_path}\" \"{CWD}/work/original/{source_path.name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ddfebc-8775-41aa-82f2-d08695914fcc",
   "metadata": {},
   "source": [
    "### 2. Superset VVs\n",
    "\n",
    "Scene frames have a tendency to move over time. This means that the extant coverage for the whole scene is always different per frame. For the cross-correlation to properly work and for more accurate comparison, all the scenes need to be \"normalized\" by increasing/decreasing the size of the square extant. \n",
    "\n",
    "From extant metadata, get the full superset coordinates for all stack scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31340f6-3946-4198-8e7e-c51f3cc7927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open all the tiffs and get overall coords.\n",
    "superset = {\n",
    "    'left': math.inf,\n",
    "    'bottom': math.inf,\n",
    "    'right': -math.inf,\n",
    "    'top': -math.inf\n",
    "}\n",
    "\n",
    "# The SRS is set to the first raster. It is assumed that the SRSs are the same (or close enough) for all.\n",
    "output_srs = None\n",
    "\n",
    "vv_original_paths = pathlib.Path(f\"{CWD}/work/original\").glob(f\"*_VV.tif\")\n",
    "\n",
    "for i, original_path in enumerate(vv_original_paths):\n",
    "\n",
    "    raster = rasterio.open(original_path)    \n",
    "    raster_bounds = raster.bounds\n",
    "    print(raster_bounds)\n",
    "    \n",
    "    if i == 0:\n",
    "        output_srs = raster.crs\n",
    "    \n",
    "    superset = {\n",
    "        'left': min(superset['left'], raster_bounds.left),\n",
    "        'bottom': min(superset['bottom'], raster_bounds.bottom), \n",
    "        'right': max(superset['right'], raster_bounds.right), \n",
    "        'top': max(superset['top'], raster_bounds.top)\n",
    "    }\n",
    "\n",
    "print(f\"Superset box coords: {superset}\")\n",
    "print(f\"Output SRS: {output_srs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebad9b9-c38f-45ca-b410-fd64e864580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any staged intermediate files to work in a clean area\n",
    "!mkdir -p \"{CWD}/work/superset\"\n",
    "for filepath in pathlib.Path(f\"{CWD}/work/superset\").glob(\"*.tif\"):\n",
    "    filepath.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6229d-3de8-4d11-ab62-897b2a64c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bounds = (\n",
    "            superset['left'], \n",
    "            superset['bottom'],\n",
    "            superset['right'],\n",
    "            superset['top'],\n",
    "        )\n",
    "\n",
    "print(f\"Output bounds (superset) set to '{output_bounds}'\")\n",
    "print(f\"Output SRS set to '{output_srs}'\")\n",
    "\n",
    "# Superset and save VVs\n",
    "vv_original_paths = pathlib.Path(f\"{CWD}/work/original\").glob(f\"*_VV.tif\")\n",
    "for original_path in vv_original_paths:\n",
    "    \n",
    "    superset_path = pathlib.Path(str(original_path).replace('original', 'superset'))\n",
    "    print(f\"Taking {original_path} and supersetting to {superset_path}\")\n",
    "    \n",
    "    gdal.Warp(\n",
    "        str(superset_path),\n",
    "        str(original_path), \n",
    "        outputBounds=output_bounds,\n",
    "        outputBoundsSRS=output_srs\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064dd33-addc-4526-abd8-2f20b43a5882",
   "metadata": {},
   "source": [
    "### 3. Flatten and Save VVs\n",
    "\n",
    "Often the VVs have extraneous high and low values that make matching difficult. So we need to get rid of these and save the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b03b33-75b2-488a-a26f-3243f9c1ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any staged intermediate files to work in a clean area\n",
    "!mkdir -p \"{CWD}/work/flatten\"\n",
    "for filepath in pathlib.Path(f\"{CWD}/work/flatten\").glob(\"*.tif\"):\n",
    "    filepath.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce713694-0dc2-4dac-ad13-8ca8a77c2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Truncated values become NaNs\n",
    "    \"\"\"\n",
    "    df[df < np.nanpercentile(df, 1)] = np.nan\n",
    "    df[df > np.nanpercentile(df, 99)] = np.nan\n",
    "    return df\n",
    "\n",
    "# Flatten and save VVs\n",
    "superset_vv_paths = pathlib.Path(f\"{CWD}/work/superset\").glob(f\"*_VV.tif\")\n",
    "\n",
    "for superset_path in superset_vv_paths:\n",
    "    print(f\"Flattening {superset_path}\")\n",
    "    \n",
    "    # Convert raster to dataframe\n",
    "    raster = rasterio.open(superset_path)\n",
    "    raster_metadata = raster.meta\n",
    "\n",
    "    raster0 = raster.read(1)\n",
    "    df_superset = pd.DataFrame(raster0)\n",
    "    \n",
    "    # Flatten raster data\n",
    "    df_flatten = flatten(df_superset)\n",
    "    \n",
    "    flatten_path = pathlib.Path(str(superset_path).replace('superset', 'flatten'))\n",
    "    \n",
    "    with rasterio.open(flatten_path, 'w', **raster_metadata) as out:\n",
    "        out.write(df_flatten, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf64fa-e58e-4b07-bdfb-fcca460567ee",
   "metadata": {},
   "source": [
    "### 4. Tile and Save VVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394ba90-1ffc-47d2-94cd-62db426fb6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_cells_args(x_num: int, y_num: int) -> list:\n",
    "    \"\"\"\n",
    "    return list of dict of args for `split_into_cells` dask function callback.\n",
    "    \"\"\"\n",
    "    \n",
    "    args = []\n",
    "    for i, flatten_path in enumerate(flatten_vv_paths):\n",
    "        args.append({\n",
    "            'input_number': i, \n",
    "            'input_file': flatten_path, \n",
    "            'output_dir': f\"{CWD}/work/tiles\", \n",
    "            'x_num': x_num, \n",
    "            'y_num': y_num\n",
    "        })\n",
    "    \n",
    "    return args \n",
    "\n",
    "# https://gis.stackexchange.com/a/306862\n",
    "# Takes a Rasterio dataset and splits it into squares of dimensions squareDim * squareDim\n",
    "def split_into_cells(args):\n",
    "    \"\"\"\n",
    "    input_number: A sequential number representing the ordering of the scenes. This is to make later scene pairing easier.\n",
    "    input_file: Full file path of scene to be tiled.\n",
    "    output_dir: Full path of directory to place tiles.\n",
    "    x_num: Number of tiles formed in the x direction per scene.\n",
    "    y_num: Number of tiles formed in the y direction per scene.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_number: int = args['input_number']\n",
    "    input_file: str = args['input_file']\n",
    "    output_dir: str = args['output_dir']\n",
    "    x_num: int = args.get('x_num', 1)\n",
    "    y_num: int = args.get('y_num', 1)\n",
    "    \n",
    "    print(f\"Tileing {input_file}\")\n",
    "\n",
    "    \n",
    "    raster = rasterio.open(input_file)\n",
    "    \n",
    "    x_dim = raster.shape[1] // x_num\n",
    "    y_dim = raster.shape[0] // y_num\n",
    "\n",
    "    x, y = 0, 0\n",
    "    for y_iter in range(y_num):\n",
    "        y = y_iter * y_dim\n",
    "        for x_iter in range(x_num):\n",
    "            x = x_iter * x_dim\n",
    "            \n",
    "            input_filestem = pathlib.Path(input_file).stem\n",
    "            \n",
    "            output_file = f'{input_filestem}_{input_number}_{y_iter}_{x_iter}.tif'\n",
    "            print(f\"Creating tile {output_file}...\")\n",
    "            \n",
    "            # Get tile geometry\n",
    "            corner1 = raster.transform * (x, y)\n",
    "            corner2 = raster.transform * (x + x_dim, y + y_dim)\n",
    "            geom = geometry.box(corner1[0], corner1[1], corner2[0], corner2[1])\n",
    "            \n",
    "            # Get cell \n",
    "            crop, cropTransform = mask(raster, [geom], crop=True)\n",
    "            raster.meta.update(\n",
    "                {\n",
    "                    \"driver\": \"GTiff\",\n",
    "                    \"height\": crop.shape[1],\n",
    "                    \"width\": crop.shape[2],\n",
    "                    \"transform\": cropTransform,\n",
    "                    \"crs\": raster.crs\n",
    "                }\n",
    "            )\n",
    "                        \n",
    "            output_filepath = f\"{output_dir}/{output_file}\"\n",
    "            with rasterio.open(output_filepath, \"w\", **raster.meta) as out:\n",
    "                out.write(crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf1305-e19a-4472-bc57-750f04d60d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any staged intermediate files to work in a clean area\n",
    "!mkdir -p \"{CWD}/work/tiles\"\n",
    "for filepath in pathlib.Path(f\"{CWD}/work/tiles\").glob(\"*.tif\"):\n",
    "    filepath.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c818293-dfcd-4243-b3db-a9610319ef45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_NUM = 8\n",
    "Y_NUM = 8\n",
    "\n",
    "flatten_vv_paths = pathlib.Path(f\"{CWD}/work/flatten\").glob(f\"*_VV.tif\")\n",
    "start_time = datetime.now()\n",
    "print(f\"\\nStart time is {start_time}\")\n",
    "\n",
    "client = setup_dask(ram_per_worker_gb=20, num_workers=100, num_threads_per_worker=1)\n",
    "do_dask(client, split_into_cells, split_into_cells_args(x_num=X_NUM, y_num=Y_NUM))\n",
    "\n",
    "teardown_dask(client)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"\\nEnd time is {end_time}\")\n",
    "print(f\"Time elapsed is {end_time - start_time}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c8f60-94de-462d-862c-7995dc61ee46",
   "metadata": {},
   "source": [
    "### 5. Correlate Tiles and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31e35a-3b89-4f21-bc5a-db3708234901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_args() -> list:\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'reference_index': '',\n",
    "            'secondary_index': '',\n",
    "            'tile_number_x': '',\n",
    "            'tile_number_y': '',\n",
    "            'ref_file_path': '',\n",
    "            'sec_file_path': ''\n",
    "        },\n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    tiles_paths = pathlib.Path(f\"{CWD}/work/tiles\").glob(f\"*.tif\")\n",
    "    tiles = []\n",
    "    \n",
    "    # Get index and tile numbers from path\n",
    "    for tiles_path in tiles_paths:\n",
    "\n",
    "        m = re.match(r\".*_([0-9]+)_([0-9]+)_([0-9]+).tif\", tiles_path.name)\n",
    "\n",
    "        tiles.append({\n",
    "            'index': m.group(1),\n",
    "            'tile_number_x': m.group(2),\n",
    "            'tile_number_y': m.group(3),\n",
    "            'file_path': tiles_path\n",
    "        })\n",
    "\n",
    "    tiles_df = pd.DataFrame(tiles).sort_values(by=['tile_number_x', 'tile_number_y', 'index'])\n",
    "\n",
    "    paris = []\n",
    "    for i in range(len(tiles_df.index) - 1):\n",
    "\n",
    "        #if i > 10:\n",
    "        #    continue\n",
    "        \n",
    "        ref_row = tiles_df.iloc[i]\n",
    "        sec_row = tiles_df.iloc[i+1]\n",
    "\n",
    "        # If the next row in the sorted dataframe has different tile numbers, then we are at a new set\n",
    "        if ref_row['tile_number_x'] != sec_row['tile_number_x'] or ref_row['tile_number_y'] != sec_row['tile_number_y']:\n",
    "            continue\n",
    "\n",
    "        paris.append({\n",
    "            'reference_index': ref_row['index'],\n",
    "            'secondary_index': sec_row['index'],\n",
    "            'tile_number_x': ref_row['tile_number_x'],\n",
    "            'tile_number_y': ref_row['tile_number_y'],\n",
    "            'ref_file_path': ref_row['file_path'],\n",
    "            'sec_file_path': sec_row['file_path']\n",
    "        })\n",
    "\n",
    "    return paris\n",
    "\n",
    "def correlation_callback(args: dict) -> dict:\n",
    "    \"\"\"\n",
    "    args = {\n",
    "        'reference_index': '',\n",
    "        'secondary_index': '',\n",
    "        'tile_number_x': '',\n",
    "        'tile_number_y': '',\n",
    "        'ref_file_path': '',\n",
    "        'sec_file_path': ''\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        reference_index = args['reference_index']\n",
    "        secondary_index = args['secondary_index']\n",
    "        tile_number_x = args['tile_number_x']\n",
    "        tile_number_y = args['tile_number_y']\n",
    "        ref_file_path = args['ref_file_path']\n",
    "        sec_file_path = args['sec_file_path']\n",
    "        \n",
    "        ###### Reference \n",
    "        stime = datetime.now()\n",
    "        print(f\"\\nIndex {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Rendering {ref_file_path}...\")\n",
    "        rast = rasterio.open(ref_file_path)\n",
    "        raster0 = rast.read(1)\n",
    "        df_ref = pd.DataFrame(raster0)\n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Time to complete ref: {datetime.now() - stime}\")\n",
    "\n",
    "\n",
    "        ###### Secondary\n",
    "        stime = datetime.now()\n",
    "        print(f\"\\nIndex {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Rendering {sec_file_path}...\")\n",
    "        rast = rasterio.open(sec_file_path)\n",
    "        raster0 = rast.read(1)\n",
    "        df_sec = pd.DataFrame(raster0)\n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Time to complete sec: {datetime.now() - stime}\")\n",
    "\n",
    "\n",
    "        ###### If crop tile is more than 10% NANs, skip correlation and set return values to NaN \n",
    "        def get_percent_nans(df):\n",
    "            number_of_elements = df.size\n",
    "            number_of_nans = df.isnull().sum().sum()\n",
    "\n",
    "            return number_of_nans / number_of_elements\n",
    "\n",
    "        percent_nans_ref = get_percent_nans(df_ref)\n",
    "        percent_nans_sec = get_percent_nans(df_sec)\n",
    "\n",
    "        if percent_nans_ref > 0.10 or percent_nans_sec > 0.10:\n",
    "            print(f\"\\nIndex {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Too many NaNs. Skipping correlation....\")\n",
    "\n",
    "            result = {\n",
    "                \"reference_index\": int(reference_index),\n",
    "                \"secondary_index\": int(secondary_index),\n",
    "                \"tile_number_x\": int(tile_number_x),\n",
    "                \"tile_number_y\": int(tile_number_y),\n",
    "                \"ref_file\": str(ref_file_path),\n",
    "                \"sec_file\": str(sec_file_path),\n",
    "                \"shift_x\": np.nan,\n",
    "                \"shift_y\": np.nan, \n",
    "                \"error\": np.nan, \n",
    "                \"phase\": np.nan,\n",
    "                \"message\": \"Too many NaNs\"\n",
    "            }\n",
    "\n",
    "        ####### Cross corr without masking\n",
    "        stime = datetime.now()\n",
    "        print(f\"\\nIndex {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Finding phase correlation with nans set to zero....\")\n",
    "        shift, error, phase = phase_cross_correlation(\n",
    "            df_ref.replace(np.nan, 0), \n",
    "            df_sec.replace(np.nan, 0),\n",
    "            normalization=None,\n",
    "            upsample_factor=10\n",
    "        )\n",
    "        \n",
    "        shift = shift * METERS_PER_PIXEL\n",
    "        error = error * METERS_PER_PIXEL\n",
    "        \n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Shift vector (in meters) required to register moving_image with reference_image: {shift}\")\n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Translation invariant normalized RMS error between reference_image and moving_image: {error}\")\n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}: Global phase difference between the two images (should be zero if images are non-negative).: {phase}\\n\")\n",
    "\n",
    "        if len(list(shift)) != 2:\n",
    "            result = {\n",
    "                \"reference_index\": int(reference_index),\n",
    "                \"secondary_index\": int(secondary_index),\n",
    "                \"tile_number_x\": int(tile_number_x),\n",
    "                \"tile_number_y\": int(tile_number_y),\n",
    "                \"ref_file\": str(ref_file_path),\n",
    "                \"sec_file\": str(sec_file_path),\n",
    "                \"shift_x\": np.float64(shift[0]),\n",
    "                \"shift_y\": np.float64(shift[1]),\n",
    "                \"error\": np.nan, \n",
    "                \"phase\": np.nan,\n",
    "                \"message\": \"Shift is not a two element array\"\n",
    "            }\n",
    "        \n",
    "        print(f\"Index {reference_index} {secondary_index}, Tile {tile_number_x} {tile_number_y}:  Time to complete correlation: {datetime.now() - stime}\")\n",
    "\n",
    "\n",
    "        ####### Write metadata to correlation result files\n",
    "\n",
    "        result = {\n",
    "            \"reference_index\": int(reference_index),\n",
    "            \"secondary_index\": int(secondary_index),\n",
    "            \"tile_number_x\": int(tile_number_x),\n",
    "            \"tile_number_y\": int(tile_number_y),\n",
    "            \"ref_file\": str(ref_file_path),\n",
    "            \"sec_file\": str(sec_file_path),\n",
    "            \"shift_x\": np.float64(shift[0]),\n",
    "            \"shift_y\": np.float64(shift[1]),\n",
    "            \"error\": np.float64(error), \n",
    "            \"phase\": np.float64(phase),\n",
    "            \"message\": \"Correlation successful\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        result = {\n",
    "            \"reference_index\": int(reference_index),\n",
    "            \"secondary_index\": int(secondary_index),\n",
    "            \"tile_number_x\": int(tile_number_x),\n",
    "            \"tile_number_y\": int(tile_number_y),\n",
    "            \"ref_file\": str(ref_file_path),\n",
    "            \"sec_file\": str(sec_file_path),\n",
    "            \"shift_x\": np.nan, \n",
    "            \"shift_y\": np.nan,\n",
    "            \"error\": np.nan, \n",
    "            \"phase\": np.nan,\n",
    "            \"message\": f\"Error: {e}\"\n",
    "        }\n",
    "        \n",
    "    try:\n",
    "        result_file = pathlib.Path(f\"{CWD}/work/correlation/index_{reference_index}_{secondary_index}-tile_{tile_number_x}_{tile_number_y}.json\")\n",
    "        with open(result_file, 'w') as f:\n",
    "            json.dump(result, f)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bcbfc5-4d9f-4459-9acf-8427b3ef3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any staged intermediate files to work in a clean area\n",
    "!mkdir -p \"{CWD}/work/correlation\"\n",
    "for filepath in pathlib.Path(f\"{CWD}/work/correlation\").glob(\"*.json\"):\n",
    "    filepath.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed10930-54c8-40b0-ac0a-270077af4360",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f\"\\nStart time is {start_time}\")\n",
    "\n",
    "# ram_per_worker_gb:int=20, num_workers:int=20, num_threads_per_worker:int=1\n",
    "client = setup_dask(ram_per_worker_gb=11, num_workers=10)\n",
    "do_dask(client, correlation_callback, get_correlation_args())\n",
    "\n",
    "teardown_dask(client)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"\\nEnd time is {end_time}\")\n",
    "print(f\"Time elapsed is {end_time - start_time}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c76bba-9d9b-4a18-9feb-19edd74ede06",
   "metadata": {},
   "source": [
    "### 6. Do Analysis on Tiles\n",
    "\n",
    "Read the correlation result files from the previous section into a Pandas DataFrame.\n",
    "\n",
    "Then perform various statistical analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f86ad8-b5d5-4ad5-b740-c069e14dfdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put results into 3D Pandas dataset\n",
    "correlation_paths = pathlib.Path(f\"{CWD}/work/correlation\").glob(f\"*.json\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for corr_path in correlation_paths:\n",
    "    with open(corr_path, 'r') as f:\n",
    "        results.append(json.load(f))\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621e8b4-43af-4165-955b-68a09540639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066c160-3a6d-45ae-9afe-914458c93898",
   "metadata": {},
   "source": [
    "Display the stats for the shift, error, and phase of the cross-correlation between two scenes in the stack.\n",
    "\n",
    "The `reference_index` is the order number of the reference scene within the stack. The stack scenes are ordered from newest to oldest.\n",
    "The `secondary_index` is the order number of the secondary scene within the stack.\n",
    "\n",
    "The `mean` value is a simple mean of all tile values. Similarity for `std`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae015a0-6ead-4ce6-b732-3deb103475e0",
   "metadata": {},
   "source": [
    "#### A. Combine all tiles per scene pair correlation results\n",
    "\n",
    "The cross-correlation results of all tiles in Scene 1 and Scene 2 are re-assembled together into one result and statistically analyzed. Repeat for all pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70568c6e-088b-4703-b52b-470ff6de1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_df = results_df.groupby(by=['reference_index'])\n",
    "\n",
    "## Uncomment to display statistical descriptions of DataFrames\n",
    "#display(paired_df['shift_x'].describe())\n",
    "#display(paired_df['shift_y'].describe())\n",
    "#display(paired_df['error'].describe())\n",
    "\n",
    "display(paired_df)\n",
    "\n",
    "# Take the Root Mean Square Error of the individual tile RMSE to get the overall RMSE.\n",
    "def rms(series):\n",
    "    if np.isnan(series).all():\n",
    "        return np.nan  \n",
    "    return np.sqrt(np.nanmean(np.square(series)))\n",
    "    #return np.nanmean(np.abs(series))\n",
    "\n",
    "pdf = pd.DataFrame()\n",
    "pdf['tile_mean_x'] = paired_df['shift_x'].agg(['mean'])\n",
    "pdf['tile_mean_y'] = paired_df['shift_y'].agg(['mean'])\n",
    "pdf['error'] = paired_df['error'].agg(rms)\n",
    "display(pdf)\n",
    "\n",
    "plt.grid(color='grey', alpha=0.4)\n",
    "plt.errorbar(pdf['tile_mean_x'], pdf['tile_mean_y'], yerr=pdf['error'], xerr=pdf['error'], ecolor='grey', alpha=0.6, ls='none')\n",
    "plt.scatter(pdf['tile_mean_x'], pdf['tile_mean_y'], color='black')\n",
    "plt.xlabel(\"X Offset (Meters)\")\n",
    "plt.ylabel(\"Y Offset (Meters)\")\n",
    "plt.title(\"Cross-correlation Offset Per Stack Pair w/ RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81335419-aca6-46f4-90c8-e605da1ee5a2",
   "metadata": {},
   "source": [
    "#### B. Combine all tile correlation results temporally\n",
    "\n",
    "The cross-correlation result of each individual tile for Scene 1 and Scene 2 are combined together temporally with the corresponding tile in later pairs. This creates a time series by tile. Repeat for all tiles. This is statistically analyzed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f43338-f522-4b9b-8d6a-b449d4e73435",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_df = results_df.groupby(by=['tile_number_x', 'tile_number_y'])\n",
    "\n",
    "## Uncomment to display statistical descriptions of DataFrames\n",
    "#display(temporal_df['shift_x'].describe())\n",
    "#display(temporal_df['shift_y'].describe())\n",
    "#display(temporal_df['error'].describe())\n",
    "\n",
    "def rms(series):\n",
    "    if np.isnan(series).all():\n",
    "        return np.nan  \n",
    "    return np.sqrt(np.nanmean(np.square(series)))\n",
    "    #return np.nanmean(np.abs(series))\n",
    "\n",
    "tdf = pd.DataFrame()\n",
    "tdf['tile_mean_x'] = temporal_df['shift_x'].agg(['mean'])\n",
    "tdf['tile_mean_y'] = temporal_df['shift_y'].agg(['mean'])\n",
    "tdf['error'] = temporal_df['error'].agg(rms)\n",
    "display(tdf)\n",
    "\n",
    "plt.grid(color='grey', alpha=0.4)\n",
    "plt.errorbar(tdf['tile_mean_x'], tdf['tile_mean_y'], yerr=tdf['error'], xerr=tdf['error'], ecolor='grey', alpha=0.6, ls='none')\n",
    "plt.scatter(tdf['tile_mean_x'], tdf['tile_mean_y'], color='black')\n",
    "plt.xlabel(\"X Offset (Meters)\")\n",
    "plt.ylabel(\"Y Offset (Meters)\")\n",
    "plt.title(\"Cross-correlation Offset Per Tile For Whole Stack w/ RMSE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opera-rtc-cross-corr [conda env:.local-opera-rtc-cross-corr]",
   "language": "python",
   "name": "conda-env-.local-opera-rtc-cross-corr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
