{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e5669e-b4fa-463b-ac40-1e40974a7e57",
   "metadata": {},
   "source": [
    "# Backscatter Distributions by Slope (Foreslope, Backslope, Flat)\n",
    "\n",
    "**Alex Lewandowski; Alaska Satellite Facility, University of Alaska Fairbanks**\n",
    "\n",
    "## Plots the RTC backscatter distributions of each slope category of every MGRS tile and polarization\n",
    "\n",
    "**Notebook Requires**\n",
    "- MGRS tiles of prepared OPERA RTC CalVal data created with Prep_OPERA_RTC_CalVal_data_stage1_part3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13588c8b-f971-44bd-8414-ef09280351e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from ipyfilechooser import FileChooser\n",
    "import numpy.ma as ma\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import rioxarray as rxr\n",
    "from scipy import stats\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "\n",
    "import opensarlab_lib as osl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db586f-d189-49c1-bb07-be2c5fbdc707",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Select whether you will be downloading data or accessing data already stored on your volume.\")\n",
    "sources = ['Download Data from S3 Bucket', 'Access Locally Stored Data']\n",
    "data_source = osl.select_parameter(sources)\n",
    "display(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0ab7a-46fe-4fce-aaa3-a656e8d93b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = data_source.value == sources[0]\n",
    "local = data_source.value == sources[1]\n",
    "\n",
    "if local:\n",
    "    print(\"Select the directory holding your MGRS tile sub-directories\")\n",
    "    fc = FileChooser(Path.cwd())\n",
    "    display(fc)\n",
    "elif s3:\n",
    "    sites = {\n",
    "        \"Site 1 (Amazon Rainforest), Descending Orbit, Summer\": \"mgrs_site1_desc.zip\",\n",
    "        \"Site 2 (Southern California), Ascending Orbit, Summer\": \"mgrs_site2_asc.zip\",\n",
    "        \"Site 2 (Southern California), Descending Orbit, Summer\": \"mgrs_site2_desc.zip\",\n",
    "        \"Site 3 (Interior Alaska), Ascending Orbit, Summer\": \"mgrs_site3_asc_summer.zip\",\n",
    "        \"Site 3 (Interior Alaska), Ascending Orbit, Winter\": \"mgrs_site3_asc_winter.zip\",\n",
    "        \"Site 3 (Interior Alaska), Descending Orbit, Summer\": \"mgrs_site3_desc_summer.zip\",\n",
    "        \"Site 3 (Interior Alaska), Descending Orbit, Winter\": \"mgrs_site3_desc_winter.zip\"\n",
    "    }\n",
    "    \n",
    "    landcovers = {\n",
    "        \"Forest\": \"forest\",\n",
    "        \"Shrub\": \"shrub\",\n",
    "        \"Herbaceous Vegetation\": \"herbs\",\n",
    "        \"Agriculture\": \"agriculture\"\n",
    "    }\n",
    "    \n",
    "    print(\"Select a dataset:\")\n",
    "    site = osl.select_parameter(sites)\n",
    "    display(site)\n",
    "    \n",
    "    print(\"\\nSelect a land cover classification:\")\n",
    "    landcover = osl.select_parameter(landcovers)\n",
    "    display(landcover)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0024ad64-8d55-4a02-b7e3-8c13549b005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local: \n",
    "    data_dir = Path(fc.selected_path)\n",
    "elif s3:\n",
    "    s3_pth = \"s3://asf-jupyter-data-west/OPERA_CalVal/Prepared_MGRS_Tiles\"\n",
    "    s3_pth = f\"{s3_pth}/{landcover.value}/{landcover.value}_{site.value}\"\n",
    "    ds_zip = Path(s3_pth).name\n",
    "    !aws --region=us-west-2 --no-sign-request s3 cp {s3_pth} {ds_zip}\n",
    "    osl.asf_unzip(str(Path.cwd()), ds_zip)\n",
    "    Path(ds_zip).unlink()\n",
    "    data_dir = Path.cwd()/Path(ds_zip).stem\n",
    "    \n",
    "mgrs = list()\n",
    "for p in Path(data_dir).iterdir():\n",
    "    if p.is_dir():\n",
    "        mgrs.append(p)\n",
    "mgrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a631c4b-0ae1-41fd-a1c4-d9c667ac93ed",
   "metadata": {},
   "source": [
    "## Plot Backscatter Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e28c9-3a62-4041-a951-cae2e7f263fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_backscatter_distributions_by_slope(fore, back, flat, means, stds, polarization, tile, dataset_name, backscatter_minmax=None, output=None):\n",
    "            # create histograms\n",
    "            f, ax = plt.subplots(figsize=(18, 8))\n",
    "            n_bins = 200\n",
    "            colors = ['blue', 'green', 'darkorange']\n",
    "            n, bins, patches = ax.hist([fore,back,flat], n_bins, color=colors,\n",
    "                                       range=backscatter_minmax, histtype='step')\n",
    "\n",
    "            # fill 1st standard deviation for each histogram and add line at mean\n",
    "            std_colors = ['skyblue', 'lightgreen', 'orange']\n",
    "            for j, hist in enumerate(patches):\n",
    "                y_max = hist[0].get_path().get_extents().y1\n",
    "                hist_path = hist[0].get_path().vertices\n",
    "                std_hist = plt.Polygon(hist_path, color=std_colors[j], fill=True, alpha=0.2)\n",
    "                ax.add_patch(std_hist)\n",
    "                std_clip = plt.Rectangle([means[j]-stds[j],means[j]+stds[j]], stds[j]*2, y_max, \n",
    "                                          fill=True, visible=False)\n",
    "                ax.add_patch(std_clip)\n",
    "                std_hist.set_clip_path(std_clip)\n",
    "                mean_line = lines.Line2D([means[j],means[j]], [0, y_max], color=colors[j], ls='--')\n",
    "                ax.add_artist(mean_line)\n",
    "                mean_line.set_clip_path(hist[0])\n",
    "\n",
    "            annotation = AnchoredText(\n",
    "                (f\"PIXEL COUNTS:\\n\"\n",
    "                 f\"foreslope:  {np.count_nonzero(~np.isnan(fore))}\\n\"\n",
    "                 f\"backslope: {np.count_nonzero(~np.isnan(back))}\\n\"\n",
    "                 f\"flat:           {np.count_nonzero(~np.isnan(flat))}\\n\\n\"\n",
    "                 f\"MEAN:\\n\"\n",
    "                 f\"foreslope:  {fore_mean}\\n\"\n",
    "                 f\"backslope: {back_mean}\\n\"\n",
    "                 f\"flat:           {flat_mean}\\n\\n\"\n",
    "                 f\"STANDARD DEVIATION:\\n\"\n",
    "                 f\"foreslope:  {fore_std}\\n\"\n",
    "                 f\"backslope: {back_std}\\n\"\n",
    "                 f\"flat:           {flat_std}\"\n",
    "                ),\n",
    "                loc='upper left', prop=dict(size=12), frameon=True, bbox_to_anchor=(1.0,1.0), bbox_transform=ax.transAxes)\n",
    "            annotation.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "            ax.add_artist(annotation)  \n",
    "\n",
    "            # add histogram legend\n",
    "            hist_handles = [lines.Line2D([0,1], [0,0], lw=1, color=c) for c in colors]\n",
    "            hist_legend = ax.legend(handles=hist_handles, labels=['foreslope','backslope','flat'], loc='upper right')\n",
    "            ax.add_artist(hist_legend)\n",
    "\n",
    "            # add standard deviation legend\n",
    "            std_handles = [Rectangle((0,0),1,1,color=c,ec=\"k\",alpha=0.2) for c in std_colors]\n",
    "            std_legend = ax.legend(handles=std_handles, labels=['foreslope 1 std', 'backslope 1 std', 'flat 1 std'], loc='center right', bbox_to_anchor=(1,0.75))\n",
    "            ax.add_artist(std_legend)\n",
    "\n",
    "            # add mean legend\n",
    "            mean_handles = [lines.Line2D([0,0], [0,1], color=c, ls='--') for c in colors]\n",
    "            mean_legend = ax.legend(handles=mean_handles, labels=['foreslope mean', 'backslope mean', 'flat mean'], loc='center right', bbox_to_anchor=(1,0.55))\n",
    "            ax.add_artist(mean_legend)\n",
    "\n",
    "            ax.set(title=f\"Distribution of {polarization} Foreslope, Backslope, and Flat Backscatter Values\\n{dataset_name}\\nMGRS: {tile}\",\n",
    "                   xlabel='Backscatter',\n",
    "                   ylabel='Frequency')\n",
    "            if output:\n",
    "                plt.savefig(output, dpi=300, transparent='true')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16cde9f-8613-4d79-82d1-b7b7a20a34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Select the scale in which to work:\")\n",
    "scale_choice = osl.select_parameter(['log scale', 'power scale'])\n",
    "display(scale_choice)\n",
    "\n",
    "print(\"Would you like to save output plots?\")\n",
    "save_choice = osl.select_parameter([\"Save Plots\", \"Do not save plots\"])\n",
    "display(save_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89f040-e648-41ea-ad55-c148ab11052a",
   "metadata": {},
   "source": [
    "### Set thresholds for removing low and high value outliers\n",
    "\n",
    "- Identify thresholds for outliers\n",
    "- Change these values to limit the x range of the backscatter distribution histograms\n",
    "- Changing these values will NOT change the already calculated means and standard deviations\n",
    "- This will only affect the range of values plotted in the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9eace7-c241-4269-a5c9-7b770e5163eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_outlier_thresh = 0.01\n",
    "low_outlier_thresh = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a5329-a7db-4651-9b58-cc57a563359f",
   "metadata": {},
   "source": [
    "### Generate Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cc5d7-e9b4-4de0-a85e-23652e561e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save = save_choice.value == \"Save Plots\"\n",
    "\n",
    "log = scale_choice.value == 'log scale'\n",
    "pols = ['VH', 'VV']\n",
    "\n",
    "vh_total = [np.array([]), np.array([]), np.array([])]\n",
    "vv_total = [np.array([]), np.array([]), np.array([])]\n",
    "\n",
    "if save:\n",
    "    plot_dir = data_dir.parent/f\"{data_dir.name}_PLOTS\"\n",
    "    if not plot_dir.is_dir():\n",
    "        plot_dir.mkdir()\n",
    "\n",
    "vh_back = np.array(list())\n",
    "vh_fore = np.array(list())\n",
    "vv_back = np.array(list())\n",
    "vv_fore = np.array(list())\n",
    "        \n",
    "for i, m in enumerate(mgrs):\n",
    "    tile = m.stem\n",
    "    \n",
    "    for p in pols:\n",
    "        fore_pth = list(m.glob(f\"*{p}_clip_*_foreslope.tif\"))[0]\n",
    "        back_pth = list(m.glob(f\"*{p}_clip_*_backslope.tif\"))[0]\n",
    "        flat_pth = list(m.glob(f\"*{p}_clip_*_flat.tif\"))[0]\n",
    "        \n",
    "        fore = rxr.open_rasterio(str(fore_pth), masked=True).to_numpy().flatten()\n",
    "        back = rxr.open_rasterio(str(back_pth), masked=True).to_numpy().flatten()\n",
    "        flat = rxr.open_rasterio(str(flat_pth), masked=True).to_numpy().flatten()\n",
    "        \n",
    "        if np.count_nonzero(~np.isnan(fore)) < 1000 or np.count_nonzero(~np.isnan(back)) < 1000 or np.count_nonzero(~np.isnan(flat)) < 1000:\n",
    "            print(f\"Skipping Tile: {tile}\")\n",
    "            print(f\"It contains a backscatter layer with less than 1000 data points\")\n",
    "            break\n",
    "\n",
    "        if log:\n",
    "            fore = 10 * np.log10(fore)\n",
    "            back = 10 * np.log10(back)\n",
    "            flat = 10 * np.log10(flat)\n",
    "            \n",
    "        if p == 'VH':\n",
    "            vh_total[0] = np.concatenate([vh_total[0], fore])\n",
    "            vh_total[1] = np.concatenate([vh_total[1], back])\n",
    "            vh_total[2] = np.concatenate([vh_total[2], flat])\n",
    "        else:\n",
    "            vv_total[0] = np.concatenate([vv_total[0], fore])\n",
    "            vv_total[1] = np.concatenate([vv_total[1], back])\n",
    "            vv_total[2] = np.concatenate([vv_total[2], flat])            \n",
    "\n",
    "        # calculate means and standard deviations before removing outliers\n",
    "        fore_mean = np.nanmean(fore)\n",
    "        fore_std = np.nanstd(fore)\n",
    "        back_mean = np.nanmean(back)\n",
    "        back_std = np.nanstd(back)\n",
    "        flat_mean = np.nanmean(flat)\n",
    "        flat_std = np.nanstd(flat)\n",
    "    \n",
    "        # Use the outlier thresholds defined in the previous cell to find the min and max x range for the histograms\n",
    "        fore_max = np.nanquantile(fore, 1-high_outlier_thresh)\n",
    "        back_max = np.nanquantile(back, 1-high_outlier_thresh)\n",
    "        flat_max = np.nanquantile(flat, 1-high_outlier_thresh)\n",
    "        fore_min = np.nanquantile(fore, low_outlier_thresh)\n",
    "        back_min = np.nanquantile(back, low_outlier_thresh)\n",
    "        flat_min = np.nanquantile(flat, low_outlier_thresh)\n",
    "        backscatter_max = max([fore_max, back_max, flat_max])\n",
    "        backscatter_min = min([fore_min, back_min, flat_min])\n",
    "\n",
    "        # gather a list of means for a pairwise T-test from outlier sanitized datasets\n",
    "        if p == 'VH':\n",
    "            vh_fore = np.append(vh_fore, fore)\n",
    "            vh_back = np.append(vh_back, back)\n",
    "        else:\n",
    "            vv_fore = np.append(vv_fore, fore)\n",
    "            vv_back = np.append(vv_back, back)     \n",
    "        \n",
    "        means = [fore_mean, back_mean, flat_mean]\n",
    "        stds = [fore_std, back_std, flat_std]\n",
    "        \n",
    "        if save:\n",
    "            output = m.parents[1]/f\"{m.parents[0].relative_to(m.parents[1])}_PLOTS/{m.stem}_{p}\"\n",
    "        else:\n",
    "            output = None\n",
    "        plot_backscatter_distributions_by_slope(fore, back, flat, means, stds, p, tile, data_dir.stem, [backscatter_min, backscatter_max], output)\n",
    "    \n",
    "    # uncomment following 2 lines for development if stopping after 1st plot\n",
    "    # if i == 0:\n",
    "    #     break\n",
    "        \n",
    "# calculate means and standard deviations for full scene\n",
    "vh_means = [np.nanmean(vh_total[0]), np.nanmean(vh_total[1]), np.nanmean(vh_total[2])]\n",
    "vv_means = [np.nanmean(vv_total[0]), np.nanmean(vv_total[1]), np.nanmean(vv_total[2])]\n",
    "vh_stds = [np.nanstd(vh_total[0]), np.nanstd(vh_total[1]), np.nanstd(vh_total[2])]\n",
    "vv_stds = [np.nanstd(vv_total[0]), np.nanstd(vv_total[1]), np.nanstd(vv_total[2])]\n",
    "\n",
    "# Use the outlier thresholds defined in the previous cell to find the min and max x range for the summary histograms\n",
    "vh_max = max([np.nanquantile(b, 1-high_outlier_thresh) for b in vh_total])\n",
    "vh_min = min([np.nanquantile(b, low_outlier_thresh) for b in vh_total])\n",
    "vv_max = max([np.nanquantile(b, 1-high_outlier_thresh) for b in vv_total])\n",
    "vv_min = min([np.nanquantile(b, low_outlier_thresh) for b in vv_total])\n",
    "\n",
    "if save:\n",
    "    vh_output = f\"{plot_dir}/full_scene_VH_PLOT\"\n",
    "    vv_output = f\"{plot_dir}/full_scene_VV_PLOT\"\n",
    "else:\n",
    "    vh_output = None\n",
    "    vv_output = None\n",
    "    \n",
    "plot_backscatter_distributions_by_slope(vh_total[0], vh_total[1], vh_total[2], vh_means, vh_stds, 'FULL SCENE VH', [m.stem for m in mgrs], \n",
    "                                        data_dir.stem, [vh_min,vh_max], vh_output)\n",
    "plot_backscatter_distributions_by_slope(vv_total[0], vv_total[1], vv_total[2], vv_means, vv_stds, 'FULL SCENE VV', [m.stem for m in mgrs], \n",
    "                                        data_dir.stem, [vv_min,vv_max], vv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061b664-e336-4be2-b27d-402a1315b4be",
   "metadata": {},
   "source": [
    "## Determine whether or not to remove outliers for T-Testing\n",
    "\n",
    "- if removing outliers\n",
    "    - define outlier thresholds\n",
    "    - create arrays absent the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb483d-0704-4f5d-afd2-d973afe84802",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"When performing T-Tests, do you wish to remove or keep outliers?\")\n",
    "outlier_choice = osl.select_parameter([\"Keep Outliers\", \"Remove Outliers\"])\n",
    "display(outlier_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9e46d-ebe2-4f5a-b305-be73d21a637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outliers = outlier_choice.value == \"Remove Outliers\"\n",
    "\n",
    "if remove_outliers:\n",
    "    print(\"Outlier thresholds are given as a decimal percentage\")\n",
    "    print(\"A low value threshold of 0.01 would remove values in the 1% quantile\\n\")\n",
    "    print(\"A high value threshold of 0.01 would remove values in the 99% quantile\\n\")\n",
    "    low_outlier_thresh = float(input(\"Enter a low value outlier threshold\"))\n",
    "    high_outlier_thresh = float(input(\"Enter a high value outlier threshold\"))                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d92b2-7cbc-41f8-831c-2174b457ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_outliers:\n",
    "    vh_fore_max = np.nanquantile(vh_fore, 1-high_outlier_thresh)\n",
    "    vh_back_max = np.nanquantile(vh_back, 1-high_outlier_thresh)\n",
    "    vh_fore_min = np.nanquantile(vh_fore, low_outlier_thresh)\n",
    "    vh_back_min = np.nanquantile(vh_back, low_outlier_thresh)\n",
    "    vh_max = max([vh_fore_max, vh_back_max])\n",
    "    vh_min = min([vh_fore_min, vh_back_min])\n",
    "    \n",
    "    vv_fore_max = np.nanquantile(vv_fore, 1-high_outlier_thresh)\n",
    "    vv_back_max = np.nanquantile(vv_back, 1-high_outlier_thresh)\n",
    "    vv_fore_min = np.nanquantile(vv_fore, low_outlier_thresh)\n",
    "    vv_back_min = np.nanquantile(vv_back, low_outlier_thresh)\n",
    "    vv_max = max([vv_fore_max, vv_back_max])\n",
    "    vv_min = min([vv_fore_min, vv_back_min])\n",
    "\n",
    "    vh_f = np.where(vh_fore<vh_max, vh_fore, np.nan)\n",
    "    vh_f = np.where(vh_f>vh_min, vh_f, np.nan)\n",
    "    vh_b = np.where(vh_back<vh_max, vh_back, np.nan)\n",
    "    vh_b = np.where(vh_b>vh_min, vh_b, np.nan)\n",
    "\n",
    "    vv_f = np.where(vv_fore<vv_max, vv_fore, np.nan)\n",
    "    vv_f = np.where(vv_f>vv_min, vv_f, np.nan)\n",
    "    vv_b = np.where(vv_back<vv_max, vv_back, np.nan)\n",
    "    vv_b = np.where(vv_b>vv_min, vv_b, np.nan)\n",
    "else: \n",
    "    vh_f = vh_fore\n",
    "    vh_b = vh_back\n",
    "    vv_f = vv_fore\n",
    "    vv_b = vv_back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3396639-e04e-4e81-9769-a154e1f11945",
   "metadata": {},
   "source": [
    "---\n",
    "## Subset datasets for T-tests\n",
    "\n",
    "For each polarization:\n",
    "- avoid using adjoining foreslope and backslope pixels to ensure independent sampling\n",
    "    - keep every 30th foreslope pixel value, starting at index 0\n",
    "    - keep every 30th backslope pixel value, starting at index 15\n",
    "- remove nan values from subsets\n",
    "- randomly shuffle subsets to avoid limiting samples to a geographic region\n",
    "- keep first 5000 pixels from each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc603af1-5c98-4074-bd6b-1dfdba16a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "vh_fore_subset = vh_f[::30]\n",
    "vh_fore_subset = vh_fore_subset[~np.isnan(vh_fore_subset)]\n",
    "np.random.shuffle(vh_fore_subset)\n",
    "vh_fore_subset = vh_fore_subset[:5000]\n",
    "\n",
    "vh_back_subset = vh_b[15::30]\n",
    "vh_back_subset = vh_back_subset[~np.isnan(vh_back_subset)]\n",
    "np.random.shuffle(vh_back_subset)\n",
    "vh_back_subset = vh_back_subset[:5000]\n",
    "\n",
    "vv_fore_subset = vv_f[::30]\n",
    "vv_fore_subset = vv_fore_subset[~np.isnan(vv_fore_subset)]\n",
    "np.random.shuffle(vv_fore_subset)\n",
    "vv_fore_subset = vv_fore_subset[:5000]\n",
    "\n",
    "vv_back_subset = vv_b[15::30]\n",
    "vv_back_subset = vv_back_subset[~np.isnan(vv_back_subset)]\n",
    "np.random.shuffle(vv_back_subset)\n",
    "vv_back_subset = vv_back_subset[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b021f67c-b543-44a0-a748-083f57396f35",
   "metadata": {},
   "source": [
    "---\n",
    "## Perform Shapiro-Wilk tests to confirm that the subset backscatter data are normally distributed for each polarization and slope\n",
    "\n",
    "### VH Foreslope Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b215a-14a1-46b3-bd4b-67816fc0af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.hist(vh_fore_subset, bins=200)\n",
    "ax.set(title='VH Foreslope Subset Pixel Distribution', xlabel='backscatter', ylabel='Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8b8b6-0883-4742-bfc8-7167dd319a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vh_fore_shapiro = stats.shapiro(vh_fore_subset)\n",
    "print(f\"{vh_fore_shapiro}\\n\")\n",
    "vh_fore_normal = vh_fore_shapiro.pvalue >= 0.05\n",
    "if vh_fore_normal:\n",
    "    print(f\"The VH foreslope subset backscatter values are normally distributed\")\n",
    "else:\n",
    "    print(f\"The VH foreslope subset backscatter values are NOT normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06225e53-73ee-4016-bebd-071210f70ddc",
   "metadata": {},
   "source": [
    "### VH Backslope Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab674bc-4377-4b8d-91f6-ec71319bdd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.hist(vh_back_subset, bins=200)\n",
    "ax.set(title='VH Backslope Subset Pixel Distribution', xlabel='backscatter', ylabel='Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29525912-d008-4282-bb91-127f9c83fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vh_back_shapiro = stats.shapiro(vh_back_subset)\n",
    "print(f\"{vh_back_shapiro}\\n\")\n",
    "vh_back_normal = vh_back_shapiro.pvalue >= 0.05\n",
    "if vh_back_normal:\n",
    "    print(f\"The VH backslope subset backscatter values are normally distributed\")\n",
    "else:\n",
    "    print(f\"The VH backslope subset backscatter values are NOT normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271026b-ee2e-45d2-b118-60b16243f3a2",
   "metadata": {},
   "source": [
    "### VV Foreslope Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45ecb9-bb9c-44fb-baae-0de9b9f790b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.hist(vv_fore_subset, bins=200)\n",
    "ax.set(title='VV Foreslope Subset Pixel Distribution', xlabel='backscatter', ylabel='Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b3ad1-4f0a-41a5-8988-cdf730032ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_fore_shapiro = stats.shapiro(vv_fore_subset)\n",
    "print(f\"{vv_fore_shapiro}\\n\")\n",
    "vv_fore_normal = vv_fore_shapiro.pvalue >= 0.05\n",
    "if vv_fore_normal:\n",
    "    print(f\"The VV foreslope subset backscatter values are normally distributed\")\n",
    "else:\n",
    "    print(f\"The VV foreslope subset backscatter values are NOT normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ea39e-b9ef-426b-8b47-9a9db6d7c497",
   "metadata": {},
   "source": [
    "### VV Backslope Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea32b8-dc7d-47e6-8660-4e8baf2b952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.hist(vv_back_subset, bins=200)\n",
    "ax.set(title='VV Backslope Subset Pixel Distribution', xlabel='backscatter', ylabel='Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834cf5b-25bd-48ce-8767-5b12dc3518b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vv_back_shapiro = stats.shapiro(vv_back_subset)\n",
    "print(f\"{vv_back_shapiro}\\n\")\n",
    "vv_back_normal = vv_back_shapiro.pvalue >= 0.05\n",
    "if vv_back_normal:\n",
    "    print(f\"The VV backslope subset backscatter values are normally distributed\")\n",
    "else:\n",
    "    print(f\"The VV backslope subset backscatter values are NOT normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0a61e-4408-4e2b-ae16-2d2439ce6364",
   "metadata": {},
   "source": [
    "---\n",
    "## VH T-Tests\n",
    "\n",
    "### Print some general sample stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf8f4c-3cbe-4c75-8ead-ee36810c5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"vh_fore_subset:\\n{stats.describe(vh_fore_subset)}\")\n",
    "print(f\"\\nvh_back_subset:\\n{stats.describe(vh_back_subset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ef365-3caf-4b0a-b562-594a8adc23fc",
   "metadata": {},
   "source": [
    "### VH T-test for means of two independent samples from descriptive statistics.\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind_from_stats.html#scipy.stats.ttest_ind_from_stats\n",
    "\n",
    "- This is a test for the null hypothesis that two independent samples have identical average (expected) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e272f-e4d6-46b9-ad06-040629d8f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind_from_stats(np.mean(vh_fore_subset), np.std(vh_fore_subset), len(vh_fore_subset), \n",
    "                           np.mean(vh_back_subset), np.std(vh_back_subset), len(vh_back_subset), \n",
    "                           equal_var=False, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc755ac-fe69-46aa-b673-0d0173775e30",
   "metadata": {},
   "source": [
    "### VH T-test for the means of two independent samples.\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
    "\n",
    "- This is a test for the null hypothesis that 2 independent samples have identical average (expected) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad5cd0-24e5-449f-8de2-7a5ee3292c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(vh_fore_subset, vh_back_subset, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f57629-f581-4d52-ad4d-4f7cad406298",
   "metadata": {},
   "source": [
    "### VH T-test on TWO RELATED samples of scores, a and b.\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html\n",
    "\n",
    "- This is a test for the null hypothesis that two related or repeated samples have identical average (expected) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188aae33-6d94-4651-b6fc-a6b5a6994ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(vh_fore_subset, vh_back_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb86fe0-6f2c-46a5-8bd9-ac69758a4e5b",
   "metadata": {},
   "source": [
    "---\n",
    "## VV T-Tests\n",
    "\n",
    "### Print some general sample stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9adf88-9215-4315-8be0-c4488e239a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"vv_fore_subset:\\n{stats.describe(vv_fore_subset)}\")\n",
    "print(f\"\\nvv_back_subset:\\n{stats.describe(vv_back_subset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a8e4b-59cd-47ff-905a-f328050cbe7a",
   "metadata": {},
   "source": [
    "### VV T-test for means of two independent samples from descriptive statistics.\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind_from_stats.html#scipy.stats.ttest_ind_from_stats\n",
    "\n",
    "- This is a test for the null hypothesis that two independent samples have identical average (expected) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98cbb0-dff7-48a2-9b1e-870b3e22e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind_from_stats(np.mean(vv_fore_subset), np.std(vv_fore_subset), len(vv_fore_subset), \n",
    "                           np.mean(vv_back_subset), np.std(vv_back_subset), len(vv_back_subset), \n",
    "                           equal_var=False, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441962c9-75df-430f-9d1a-ca73b25605b4",
   "metadata": {},
   "source": [
    "### VV T-test for the means of two independent samples.\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n",
    "\n",
    "- This is a test for the null hypothesis that 2 independent samples have identical average (expected) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90365c-b113-45c9-b071-6c0183077b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(vv_fore_subset, vv_back_subset, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697895b-e221-458e-9384-5b93fcf766b6",
   "metadata": {},
   "source": [
    "### VV T-test on TWO RELATED samples of scores, a and b.\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html\n",
    "\n",
    "- This is a test for the null hypothesis that two related or repeated samples have identical average (expected) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464f364-0b60-467d-9179-3daf816a815c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats.ttest_rel(vv_fore_subset, vv_back_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4618f-952a-4d9a-8d76-dc92d1471856",
   "metadata": {},
   "source": [
    "*Backscatter_Distributions_by_Slope - Version 0.1.0 - April 2022*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtc_analysis",
   "language": "python",
   "name": "conda-env-.local-rtc_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
